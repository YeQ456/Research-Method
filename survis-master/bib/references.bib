@article{arulkumaran2017brief,
  abstract = {Deep reinforcement learning (DRL) is poised to revolutionize the field of artificial intelligence (AI) and represents a step toward building autonomous systems with a higher-level understanding of the visual world. Currently, deep learning is enabling reinforcement learning (RL) to scale to problems that were previously intractable, such as learning to play video games directly from pixels. DRL algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of RL, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep RL, including the deep Q-network (DQN), trust region policy optimization (TRPO), and asynchronous advantage actor critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via RL. To conclude, we describe several current areas of research within the field.},
  keywords = {type: Deep Learning, Reinforcement Learning, Autonomous Systems, Visual Understanding, Policy Search, Value Functions, Deep Nerual Networks, Algorithms, Robotics Control, Games, Visual Navigation, Model Predictive Control},
  title = {A brief survey of deep reinforcement learning},
  doi = {10.1109/MSP.2017.2743240},
  series = {MSP},
  piblisher = {IEEE},
  volume = {34},
  number = {01},
  pages = {26--38},
  author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  journal = {IEEE Signal Processing Magazine},
  url = {https://doi.org/10.1109/MSP.2017.2743240},
  year = {2017}
}


@article{chen2022redeeming,
  title = {Redeeming intrinsic rewards via constrained optimization},
  abstract = {State-of-the-art reinforcement learning (RL) algorithms typically use random sampling (e.g., 
ϵ
-greedy) for exploration, but this method fails on hard exploration tasks like Montezuma's Revenge. To address the challenge of exploration, prior works incentivize exploration by rewarding the agent when it visits novel states. Such intrinsic rewards (also called exploration bonus or curiosity) often lead to excellent performance on hard exploration tasks. However, on easy exploration tasks, the agent gets distracted by intrinsic rewards and performs unnecessary exploration even when sufficient task (also called extrinsic) reward is available. Consequently, such an overly curious agent performs worse than an agent trained with only task reward. Such inconsistency in performance across tasks prevents the widespread use of intrinsic rewards with RL algorithms. We propose a principled constrained optimization procedure called Extrinsic-Intrinsic Policy Optimization (EIPO) that automatically tunes the importance of the intrinsic reward: it suppresses the intrinsic reward when exploration is unnecessary and increases it when exploration is required. The results is superior exploration that does not require manual tuning in balancing the intrinsic reward against the task reward. Consistent performance gains across sixty-one ATARI games validate our claim. The code is available at https://github.com/Improbable-AI/eipo.},
  author = {Chen, Eric and Hong, Zhang-Wei and Pajarinen, Joni and Agrawal, Pulkit},
  keywords = {type: Reinforcement Learning, Exploration, Intrinsic Reward, Extrinsic Reward, Policy Optimization, ATARI Games, Exploration-Exploitation Dilemma},
  journal = {Advances in Neural Information Processing Systems},
  doi = {10.48550/arXiv.2211.07627},
  volume = {35},
  pages = {4996--5008},
  year = {2022},
  url = {https://dx.doi.org/10.48550/arXiv.2211.07627},
  series = {arXiv},
  number = {01}
}

@article{franccois2018introduction,
  abstract = {Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decisionmaking tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.},
  keywords = {type: Deep Learning, Reinforcement Learning, Generalisation, Sequential Decision-Making, Practical Application},
  title = {An introduction to deep reinforcement learning},
  author = {François-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G and Pineau, Joelle and others},
  journal = {Foundations and Trends{\textregistered} in Machine Learning},
  volume = {11},
  doi = {10.1561/2200000071},
  number = {3-4},
  pages = {219--354},
  year = {2018},
  publisher = {Now Publishers, Inc.},
  url = {http://dx.doi.org/10.1561/2200000071},
  issn = {1935-8237}
}

@inproceedings{ioffe2015batch,
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.},
  title = {Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  keywords = {type: deep learning, batch normalisation, covariate offset, neural network training, image classification},
  author = {Ioffe, Sergey and Szegedy, Christian},
  booktitle = {International Conference on Machine Learning},
  pages = {448--456},
  doi = {10.48550/arXiv.1502.03167},
  series = {arXiv},
  url = {https://dx.doi.org/10.48550/arXiv.1502.03167},
  year = {2015},
  organization = {pmlr},
  publisher = {JMLR: W&CP volume 37}
}

@article{kiran2021deep,
  abstract = {With the development of deep representation learning, the domain of reinforcement learning (RL) has become a powerful learning framework now capable of learning complex policies in high dimensional environments. This review summarises deep reinforcement learning (DRL) algorithms and provides a taxonomy of automated driving tasks where (D)RL methods have been employed, while addressing key computational challenges in real world deployment of autonomous driving agents. It also delineates adjacent domains such as behavior cloning, imitation learning, inverse reinforcement learning that are related but are not classical RL algorithms. The role of simulators in training agents, methods to validate, test and robustify existing solutions in RL are discussed.},
  keywords = {type: deep reinforcement learning, autonomous driving, imitation learning, inverse reinforcement learning, controller learning, trajectory optimisation, motion planning, safe reinforcement learning},
  title = {Deep reinforcement learning for autonomous driving: A survey},
  author = {Kiran, B Ravi and Sobh, Ibrahim and Talpaert, Victor and Mannion, Patrick and Al Sallab, Ahmad A and Yogamani, Senthil and Pérez, Patrick},
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {23},
  number = {6},
  pages = {4909--4926},
  year = {2021},
  publisher = {IEEE},
  doi = {10.1109/TITS.2021.3054625},
  url = {https://dx.doi.org/10.1109/TITS.2021.3054625},
  series = {TITS}
}

@article{mcgrath2022acquisition,
  abstract = {We analyze the knowledge acquired by AlphaZero, a neural network engine that learns chess solely by playing against itself yet becomes capable of outperforming human chess players. Although the system trains without access to human games or guidance, it appears to learn concepts analogous to those used by human chess players. We provide two lines of evidence. Linear probes applied to AlphaZero’s internal state enable us to quantify when and where such concepts are represented in the network. We also describe a behavioral analysis of opening play, including qualitative commentary by a former world chess champion.},
  keywords = {type: machine learning, artificial intelligence, interpretability, reinforcement learning, deep learning},
  title = {Acquisition of chess knowledge in alphazero},
  author = {McGrath, Thomas and Kapishnikov, Andrei and Toma{\v{s}}ev, Nenad and Pearce, Adam and Wattenberg, Martin and Hassabis, Demis and Kim, Been and Paquet, Ulrich and Kramnik, Vladimir},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {119},
  number = {47},
  pages = {e2206625119},
  year = {2022},
  publisher = {National Acad Sciences},
  url = {https://doi.org/10.1073/pnas.2206625119},
  doi = {10.1073/pnas.2206625119},
  series = {PANS}
}

@article{santurkar2018does,
  abstract = {Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called "internal covariate shift". In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.},
  keywords = {type: Optimising landscape smoothing, gradient behaviour, Learning rate and convergence, internal covariate offset},
  title = {How does batch normalization help optimization?},
  author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
  journal = {Advances in Neural Information Processing Systems},
  volume = {31},
  doi = {10.48550/arXiv.1805.11604},
  url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/905056c1ac1dad141560467e0a99e1cf-Paper.pdf},
  year = {2018},
  publisher = {Curran Associates, Inc.},
  booktitle = {Advances in Neural Information Processing Systems}
}

@inproceedings{van2016deep,
  abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
  keywords = {type: overestimation, Double Q-learning, Double DQN algorithm, Results},
  title = {Deep reinforcement learning with double q-learning},
  author = {Van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle = {Proceedings of the AAAI conference on artificial intelligence},
  volume = {30},
  number = {1},
  year = {2016},
  url = {https://doi.org/10.1609/aaai.v30i1.10295},
  doi = {10.1609/aaai.v30i1.10295},
  series = {AAAI},
  publisher = {Association for the Advancement of Artificial Intelligence, AAAI}
}

@article{yang2021multi,
  abstract = {A variety of meta-heuristics have shown promising performance for solving multi-objective optimization problems (MOPs). However, existing meta-heuristics may have the best performance on particular MOPs, but may not perform well on the other MOPs. To improve the cross-domain ability, this paper presents a multi-objective hyper-heuristic algorithm based on adaptive epsilon-greedy selection (HH_EG) for solving MOPs. To select and combine low-level heuristics (LLHs) during the evolutionary procedure, this paper also proposes an adaptive epsilon-greedy selection strategy. The proposed hyper-heuristic can solve problems from varied domains by simply changing LLHs without redesigning the high-level strategy. Meanwhile, HH_EG does not need to tune parameters, and is easy to be integrated with various performance indicators. We test HH_EG on the classical DTLZ test suite, the IMOP test suite, the many-objective MaF test suite, and a test suite of a real-world multi-objective problem. Experimental results show the effectiveness of HH_EG in combining the advantages of each LLH and solving cross-domain problems.},
  title = {A multi-objective hyper-heuristic algorithm based on adaptive epsilon-greedy selection},
  keywords = {type: Hyper-heuristics, Multi-objective optimization, Adaptive epsilon-greedy selection strategy, cross-domain problems},
  author = {Yang, Tailong and Zhang, Shuyan and Li, Cuixia},
  journal = {Complex \& Intelligent Systems},
  volume = {7},
  pages = {765--780},
  year = {2021},
  publisher = {Springer},
  doi = {10.1007/s40747-020-00230-8},
  url = {https://dx.doi.org/10.1007/s40747-020-00230-8},
  number = {03}
}

@article{zhu2023transfer,
  abstract = {Reinforcement learning is a learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in reinforcement learning upon the fast development of deep neural networks. Along with the promising prospects of reinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges faced by reinforcement learning, by transferring knowledge from external expertise to facilitate the efficiency and effectiveness of the learning process. In this survey, we systematically investigate the recent progress of transfer learning approaches in the context of deep reinforcement learning. Specifically, we provide a framework for categorizing the state-of-the-art transfer learning approaches, under which we analyze their goals, methodologies, compatible reinforcement learning backbones, and practical applications. We also draw connections between transfer learning and other relevant topics from the reinforcement learning perspective and explore their potential challenges that await future research progress.},
  title = {Transfer learning in deep reinforcement learning: A survey},
  author = {Zhu, Zhuangdi and Lin, Kaixiang and Jain, Anil K and Zhou, Jiayu},
  keywords = {type: Transfer learning, reinforcement learning, deep learning, survey},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2023},
  publisher = {IEEE},
  doi = {10.1109/TPAMI.2023.3292075},
  series = {TPAMI},
  url = {https://dx.doi.org/10.1109/TPAMI.2023.3292075},
  pages = {13344--13362},
  volume = {45}
}

